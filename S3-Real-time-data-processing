import json
import urllib.parse
import boto3
import csv

s3 = boto3.client('s3')

def lambda_handler(event, context):
    for record in event['Records']:
        # Get the bucket name and key (file path) of the new S3 object
        bucket_name = record['s3']['bucket']['name']
        file_key = urllib.parse.unquote_plus(record['s3']['object']['key'], encoding='utf-8')

        # Check if the file type is CSV
        if file_key.endswith('.csv'):

            # Download the CSV file from S3
            response = s3.get_object(Bucket=bucket_name, Key=file_key)
            file_content = response['Body'].read().decode('utf-8')

            # Process the CSV file data
            reader = csv.reader(file_content.split('\n'))
            
            # Do something with the CSV data here
            
            return {
                'statusCode': 200,
                'body': json.dumps('CSV file processed successfully')
            }

    return {
        'statusCode': 404,
        'body': json.dumps('No CSV files were found in the S3 bucket')
    }

To process data uploaded to an S3 bucket in real-time, you can leverage the S3 events and triggers. You can create an AWS Lambda function that is triggered by an S3 event whenever a new object is added to the specified bucket. This is an example Python code snippet that processes CSV files uploaded in real-time:

